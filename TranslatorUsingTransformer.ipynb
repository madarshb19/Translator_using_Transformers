{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madarshb19/Translator_using_Transformers/blob/main/TranslatorUsingTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDw_jQC3f1za"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import time #to know how long each epoch will take\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "8j7MXD2S1YOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc7Wf_yngJGw"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cv1Pb3xgLkL"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-X9aj9IgPbv"
      },
      "source": [
        "We import files from our personal google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\") #used to link google drive to our notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_EWu2Or73Hq",
        "outputId": "f020d571-3889-480a-bb0e-00bea988718c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Transformer/europarl-v7.fr-en.en\",\n",
        "          mode = 'r',\n",
        "          encoding = 'utf-8') as f:\n",
        "          europarl_en = f.read()"
      ],
      "metadata": {
        "id": "lcx8lWsQ8Csm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Transformer/europarl-v7.fr-en.fr\",\n",
        "          mode = 'r',\n",
        "          encoding = 'utf-8') as f:\n",
        "          europarl_fr = f.read()"
      ],
      "metadata": {
        "id": "72Tn5OWV86oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Transformer/nonbreaking_prefix.en\",\n",
        "          mode = 'r',\n",
        "          encoding = 'utf-8') as f:\n",
        "          non_breaking_prefix_en = f.read()"
      ],
      "metadata": {
        "id": "Edwh7KVN86w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Transformer/nonbreaking_prefix.fr\",\n",
        "          mode = 'r',\n",
        "          encoding = 'utf-8') as f:\n",
        "          non_breaking_prefix_fr = f.read()"
      ],
      "metadata": {
        "id": "X_0JE8_t862y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "europarl_en[:102]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gEuHj_jH9hqH",
        "outputId": "f381f846-dac1-4935-8679-2748313d52a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzAviIErhKEH"
      },
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoBBMXpUhM8U"
      },
      "source": [
        "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pPu-WJ4DBwo8",
        "outputId": "d9c24050-2128-42f5-e456-a122f76bb0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\nv\\nw\\nx\\ny\\nz\\nmessrs\\nmlle\\nmme\\nmr\\nmrs\\nms\\nph\\nprof\\nsr\\nst\\na.m\\np.m\\nvs\\ni.e\\ne.g'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8SBnCw6Ec7A",
        "outputId": "1f661319-b4e5-4efa-8cab-a3b84f6858db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " 'messrs',\n",
              " 'mlle',\n",
              " 'mme',\n",
              " 'mr',\n",
              " 'mrs',\n",
              " 'ms',\n",
              " 'ph',\n",
              " 'prof',\n",
              " 'sr',\n",
              " 'st',\n",
              " 'a.m',\n",
              " 'p.m',\n",
              " 'vs',\n",
              " 'i.e',\n",
              " 'e.g']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gju7kOXGGfI",
        "outputId": "c6f316c8-39d5-4ad2-b167-3d495cb0bf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.',\n",
              " ' b.',\n",
              " ' c.',\n",
              " ' d.',\n",
              " ' e.',\n",
              " ' f.',\n",
              " ' g.',\n",
              " ' h.',\n",
              " ' i.',\n",
              " ' j.',\n",
              " ' k.',\n",
              " ' l.',\n",
              " ' m.',\n",
              " ' n.',\n",
              " ' o.',\n",
              " ' p.',\n",
              " ' q.',\n",
              " ' r.',\n",
              " ' s.',\n",
              " ' t.',\n",
              " ' u.',\n",
              " ' v.',\n",
              " ' w.',\n",
              " ' x.',\n",
              " ' y.',\n",
              " ' z.',\n",
              " ' messrs.',\n",
              " ' mlle.',\n",
              " ' mme.',\n",
              " ' mr.',\n",
              " ' mrs.',\n",
              " ' ms.',\n",
              " ' ph.',\n",
              " ' prof.',\n",
              " ' sr.',\n",
              " ' st.',\n",
              " ' a.m.',\n",
              " ' p.m.',\n",
              " ' vs.',\n",
              " ' i.e.',\n",
              " ' e.g.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")"
      ],
      "metadata": {
        "id": "5pXbdkD6FKSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "metadata": {
        "id": "1evescHmGwEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_breaking_prefix_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L0k0Y2BGpE_",
        "outputId": "7e6d7cb3-338d-4c29-aac5-24aa398f5296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.',\n",
              " ' b.',\n",
              " ' c.',\n",
              " ' d.',\n",
              " ' e.',\n",
              " ' f.',\n",
              " ' g.',\n",
              " ' h.',\n",
              " ' i.',\n",
              " ' j.',\n",
              " ' k.',\n",
              " ' l.',\n",
              " ' m.',\n",
              " ' n.',\n",
              " ' o.',\n",
              " ' p.',\n",
              " ' q.',\n",
              " ' r.',\n",
              " ' s.',\n",
              " ' t.',\n",
              " ' u.',\n",
              " ' v.',\n",
              " ' w.',\n",
              " ' x.',\n",
              " ' y.',\n",
              " ' z.',\n",
              " ' messrs.',\n",
              " ' mlle.',\n",
              " ' mme.',\n",
              " ' mr.',\n",
              " ' mrs.',\n",
              " ' ms.',\n",
              " ' ph.',\n",
              " ' prof.',\n",
              " ' sr.',\n",
              " ' st.',\n",
              " ' a.m.',\n",
              " ' p.m.',\n",
              " ' vs.',\n",
              " ' i.e.',\n",
              " ' e.g.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRy8a6WYhl7Z"
      },
      "source": [
        "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_en = europarl_en\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix,prefix + '###') #to differentiate between full stops and i.e's dots and other examples\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_en) # add \\ because . has another meaning in regex.\n",
        "# \"\\.(?=[0-9]|[a-z]|[A-Z])\" means selectign all points which are not full stops and now we remove them and replace them with empty string\n",
        "corpus_en = re.sub(r\"\\.###\",'',corpus_en)\n",
        "corpus_en = re.sub(r\" +\",\" \",corpus_en) # => looking for more than 2 spaces replaced by 1 space\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "corpus_fr = europarl_en\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "  corpus_fr = corpus_fr.replace(prefix,prefix + '###') #to differentiate between full stops and i.e's dots and other examples\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_fr) # add \\ because . has another meaning in regex.\n",
        "# \"\\.(?=[0-9]|[a-z]|[A-Z])\" means selectign all points which are not full stops and now we remove them and replace them with empty string\n",
        "corpus_fr = re.sub(r\"\\.###\",'',corpus_fr)\n",
        "corpus_fr = re.sub(r\" +\",\" \",corpus_fr) # => looking for more than 2 spaces replaced by 1 space"
      ],
      "metadata": {
        "id": "9Kcjf-3jHTxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_fr = corpus_fr.split(\"\\n\")"
      ],
      "metadata": {
        "id": "_W6G3DoFL17e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0gJoNgL02yY"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en,target_vocab_size = 2**13) #yet to be optimized\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr,target_vocab_size = 2**13)"
      ],
      "metadata": {
        "id": "g1GZ6XlaLaZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN =  tokenizer_en.vocab_size + 2 #bcuz we will add \"words\" during evaluation\n",
        "VOCAB_SIZE_FR =  tokenizer_fr.vocab_size + 2"
      ],
      "metadata": {
        "id": "yJWN9ie9Mo3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "metadata": {
        "id": "IoJD_Px-NJMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuaPXaDg1qDQ"
      },
      "source": [
        "## Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creates problems in padding (very big MAX_LEN value); too long to train,etc...\n",
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count,sent in enumerate(inputs) if len(sent) > MAX_LENGTH] #enumerate gives count as well other than the sentence\n",
        "for idx in reversed(idx_to_remove): #we do it in reverse because if we do it forward,the later elements will already be shifted before performing the remove operation and the wrong elements will be removed\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count,sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "metadata": {
        "id": "ZhNfy2w3PYXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUxUThRH2HFf"
      },
      "source": [
        "## Inputs/outputs creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPC9nPN2M6B"
      },
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Padding"
      ],
      "metadata": {
        "id": "YYq_aez7RjdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,value = 0,padding = 'post',maxlen = MAX_LENGTH) #value = 0 as 0 not used in tokenizer\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,value = 0,padding = 'post',maxlen = MAX_LENGTH)"
      ],
      "metadata": {
        "id": "5u2KB3HHRhgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20,000 #used for shuffling of datasets\n",
        "\n",
        "dataset = tf.data.Datasets.from_tensor_slices((inputs,outputs))\n",
        "dataset = dataset.cache() #helps in storage and access to dataset during training\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "09TAGs9dSXKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn2w4DqH288u"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcFBxDRZ2_WT"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0aOFAzp3Buh"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PostionalEncoding(layers.Layer): #inherited from layers.Layer class\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding,self).__init__()\n",
        "\n",
        "  def get_angles(self,pos,i,d_model): #pos:arrays of all positions of input (0,19),dim : (seq_len,1) ; i is position of embeddings,dim:(1,d_model)\n",
        "    angles = i/np.power(1000.,(2*(i/2))/np.float_32(d_model))\n",
        "    return pos*angles #shape: (seq_len,d_model)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = input.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:,np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis,:],\n",
        "                             d_model)\n",
        "    \n",
        "    angles[:,0::2] = np.sin(angles,angles[:,0::2]) #all even indices\n",
        "    angles[:,1::2] = np.cos(angles,angles[:,1::2]) #all odd\n",
        "    pos_encoding = angles[np.newaxis,...] #... : keep everything else same\n",
        "    return inputs + tf.cast(pos_encoding,tf.float32)"
      ],
      "metadata": {
        "id": "cAoXu2vPuWA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkTpS5X4oSw"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOXSznTT4r0C"
      },
      "source": [
        "### Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VilQe5gT4uA3"
      },
      "source": [
        "$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ where Q is the query K is the keys and d_k is the key dimension"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(queries,keys,values,mask): #mask:look ahead,padding mask and zero mask\n",
        "  product = tf.matmul(queries,keys,transpose_b = True)\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1],tf.float32) #to get dimensions of keys,K.Last dimension is the embedding which we want\n",
        "  scaled_product = product/tf.math.sqrt(keys_dim)\n",
        "  #lets implement masking as well\n",
        "  if mask is not None:\n",
        "    scaled_product+= (mask * -1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product,axis = 1),values)\n",
        "  return attention\n"
      ],
      "metadata": {
        "id": "ZIgzFgAgHokn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYhP6g5i5cba"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self,nb_proj):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "    \n",
        "\n",
        "  def build(self,input_shape) : #build is similiar to init but instead of being called when an object is created,it is called when the object is used for the first time;allows us to access more information.\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.n_proj == 0 #assumes this should be 0\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "    self.query_lin = layers.Dense(units = self.d_model)\n",
        "    self.key_lin = layers.Dense(units = self.d_model)\n",
        "    self.value_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "  def split_proj(self,inputs,batch_size): #we need batch size to rehape input : (batch_size,seq_length,d_model) => (batch_size,nb_proj,seq_length,d_proj)\n",
        "    shape = (batch_size,-1,self.nb_proj,self.d_proj) #shape used to reshape\n",
        "    splitted_inputs = tf.reshape(inputs,shape = shape)  #(batch_size,seq_length,nb_proj,d_proj)\n",
        "    return tf.transpose(splitted_inputs, perm = [0,2,1,3]) #[0,1,2,3] -> [0,2,1,3]\n",
        "\n",
        "  def call(self,queries,keys,values,mask):\n",
        "\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.query_lin(queries) #following architecture\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries,batch_size) #splitting inputs\n",
        "    keys = self.split_proj(keys,batch_size)\n",
        "    values = self.split_proj(values,batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(queries,keys,values,mask)\n",
        "    attention = tf.transpose(attention,perm = [0,2,1,3])\n",
        "    concat_attention = tf.rehsape(attention,shape = (batch_size,-1,self.d_model))\n",
        "\n",
        "    output = self.final_lin(concat_attention)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "2GmmMoPJJje9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88reCtV7Vo-"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init(self,FFN_units,nb_proj,dropout):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self,input_shape): #to get dimension of models\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate = self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.dense_1 = layers.Dense(units = self.FFN_units,activation = 'relu')\n",
        "    self.dense_2 = layers.Dense(units = self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate = self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "  \n",
        "  def call(self,inputs,mask,training):\n",
        "    attention = self.multi_head_attention(inputs,inputs,inputs,mask) #bcuz from the architecture,keys values and quesries are all inputs\n",
        "\n",
        "    attention = self.dropout_1(attention,training = training)\n",
        "    attention = self.norm_1(attention+inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs)\n",
        "    outputs = self.norm_2(outputs+attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "mls5hetec193"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "  \n",
        "  def __init__(self,nb_layers,FFN_units,nb_proj,dropout,vocab_size,d_model,name = \"encoder\"):\n",
        "    super(Encoder,self).__init__(name = name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.FFN_units = FFN_units #not req\n",
        "    self.nb_proj = nb_proj #not req\n",
        "    self.dropout = dropout #not req\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.embedding(vocab_size,d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout)\n",
        "    self.enc_layer = [EncoderLayer(FFN_units,nb_proj,dropout) for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model),tf.float32) #more info in paper about this. (3.4 embedding and softmax).casting is done to convert d_model from int to float so that sqrt can be applied to it\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs,training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      output = self.enc_layers[i](outputs,mask,training)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "ABra54lOf_7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFCrh2G_-mKx"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self,FFN_units,nb_proj,dropout):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate = self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "    \n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate = self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units = self.FFN_units,activation = 'relu')\n",
        "    self.dense_2 = layers.Dense(units = self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate = self.dropout)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "  def call(self,inputs,encoutputs,mask_1,mask_2,training): #mask 1 is for the self attention layer,mask 2 is for the 2nd attention layer (see architecture)\n",
        "    attention = self.multi_head_attention_1(inputs,inputs,inputs,mask_1)\n",
        "    attention = self.dropout_1(attention,training)\n",
        "    attention = self.norm_1(attention+inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,enc_outputs,enc_outputs,mask_2) #queries from attention and keys and vlaues from outputs of encoder\n",
        "    attention_2 = self.dropout_2(attention_2,training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(output)\n",
        "    outputs = self.dropout_3(outputs,training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "ti4E9NvnjOfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self,nb_layers,FFN_units,nb_proj,dropout,vocab_size,d_model,name = \"decoder\"):\n",
        "    super(Decoder,self).__init__(name = name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "    self.embedding = layers.Embedding(vocab_size,d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units,nb_proj,dropout) for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self,inputs,enc_outputs,mask_1,mask_2,training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs = tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs  =self.dropout(outputs,training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,enc_outputs,mask_1,mask_2,training)\n",
        "    \n",
        "    return outputs\n",
        "    "
      ],
      "metadata": {
        "id": "NpMmJJTMm9Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf_YD4anAbfb"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "    \n",
        "    def create_padding_mask(self, seq): # seq: (batch_size, seq_length)\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ZRqvbax2pY1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIBSpVE_B9NA"
      },
      "source": [
        "# Stage 4: Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JYN0U3NCChb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)"
      ],
      "metadata": {
        "id": "mdKSWxGNGvOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "metadata": {
        "id": "rzn2QjI3GyVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "    \n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "YP-PeCIMG0TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./drive/MyDrive/projects/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!\")"
      ],
      "metadata": {
        "id": "l8rH3uidG2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpont for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "    print(\"time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "metadata": {
        "id": "7tT5ZMlPG8qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2vY9ogwzFW0"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False) # (1, seq_length, vocab_size_fr)\n",
        "\n",
        "        prediction = predictions[:, -1:, :]\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "metadata": {
        "id": "iLkbp02rG9qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "\n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "metadata": {
        "id": "HXpaZqElHAAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is finally done.\")"
      ],
      "metadata": {
        "id": "awnHGTvmHClZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}